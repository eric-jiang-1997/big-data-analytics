{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "#### This is one of the assignments from course CS 431/631 (Data-intensive Distributed Analytics) at University of Waterloo.\n",
    "#### This assignment focuses on the text mining to get the 'PMI' scores of specific token pairs.\n",
    "#### Some modifications have been made to improve the presentation on this platform.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "**Goal:** Use Python to analyze the [pointwise mutual information (PMI)](http://en.wikipedia.org/wiki/Pointwise_mutual_information) of tokens in the text of Shakespeare's plays.\\\n",
    "**Files needed:** the text file (`Shakespeare.txt`); the Python tokenizer module (`simple_tokenize.py`).\n",
    "\n",
    "If two events $x$ and $y$ are independent, their PMI will be zero.   A positive PMI indicates that $x$ and $y$ are more likely to co-occur than they would be if they were independent.   Similarly, a negative PMI indicates that $x$ and $y$ are less likely to co-occur.   The PMI of events $x$ and $y$ is given by\n",
    "\\begin{equation*}\n",
    "PMI(x,y) = \\log\\frac{p(x,y)}{p(x)p(y)}\n",
    "\\end{equation*}\n",
    "where $p(x)$ and $p(y)$ are the probabilities of occurrence of events $x$ and $y$, and $p(x,y)$ is the probability of co-occurrence of $x$ and $y$.\n",
    "\n",
    "For here, the \"events\" that we are interested in are occurrences of tokens on lines of text in the input file.   For example, one event\n",
    "might represent the occurence of the token \"fire\" a line of text, and another might represent the occurrence of the token \"peace\".   In that case, $p(fire)$ represents the probability that \"fire\" will occur on a line of text, and $p(fire,peace)$ represents the probability that *both* \"fire\" and \"peace\" will occur on the *same* line.   For the purposes of these PMI computations, it does not matter how many times a given token occures on a single line.   Either a line contains a particular token (at least once), or it does not.   For example, consider this line of text:\n",
    "\n",
    "> three three three, said thrice\n",
    "\n",
    "For this line, the following token-pair events have occurred:\n",
    "- (three, said)\n",
    "- (three, thrice)\n",
    "- (said, three)\n",
    "- (said, thrice)\n",
    "- (thrice, three)\n",
    "- (thrice, said)\n",
    "\n",
    "Note that we are not interested in \"reflexive\" pairs, such as (thrice,thrice).\n",
    "\n",
    "In addition to the probabilities of events, we will also be interested in the absolute *number* of occurences of particular events, e.g., the number of lines in which \"fire\" occurs.   We will use $n(x)$ to represent the these numbers.\n",
    "\n",
    "The main task is to write Python code to analyze the PMI of tokens from Shakespeare's plays.    Based this analysis, we want to be able to answer two types of queries:\n",
    "\n",
    "* Two-Token Queries: Given a pair of tokens, $x$ and $y$, report the number of lines on which that pair co-occurs ($n(x,y)$) as well as $PMI(x,y)$.\n",
    "* One-Token Queries: Given a single token, $x$, report the number of lines on which that token occurs ($n(x)$).   In addition, report the five tokens that have the largest PMI with respect to $x$ (and their PMIs).   That is, report the five $y$'s for which $PMI(x,y)$ is largest.\n",
    "\n",
    "To avoid reporting spurious results for the one-token queries, we are only interested in token pairs that co-occur a sufficient number of times.   Therefore, we will use a *threshold* parameter for one-token queries.   A one-token query should only report pairs of tokens that co-occur at least *threshold* times in the input.   For example, given the threshold 12, a one-token query for \"fire\" the should report the five tokens that have the largest PMI (with respect to \"fire\") among all tokens that co-occur with \"fire\" on at least 12 lines.   If there are fewer than five such tokens, report fewer than five.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Part 1:\n",
    "\n",
    "First, write some code to have an idea of how big the PMI analysis problem will be. Specifically, this code determines: (a) the number of *distinct* tokens that exist in 'Shakespeare.txt', and (b) the number of \n",
    "distinct token pairs that exist in 'Shakespeare.txt'.  (consider the token pair $x,y$ to be distinct from the pair $y,x$, i.e., count them both; also ignore token pairs of the form $x,x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct tokens is 25975\n",
      "The number of distinct token pairs is 1969760\n"
     ]
    }
   ],
   "source": [
    "# this imports the SimpleTokenize function from the simple_tokenize.py file\n",
    "from simple_tokenize import simple_tokenize\n",
    "\n",
    "# Now, let's tokenize Shakespeare's plays\n",
    "tokens_dic = {}\n",
    "token_pairs_dic = {}\n",
    "with open('Shakespeare.txt') as f:\n",
    "    for line in f:\n",
    "        # tokenize, one line at a time\n",
    "        t = simple_tokenize(line)\n",
    "        # for each line, get a dictionary with keys(tokens) and values(count)\n",
    "        for i in t:\n",
    "                if i not in tokens_dic:\n",
    "                    tokens_dic[i] = 1\n",
    "                else:\n",
    "                    tokens_dic[i] += 1\n",
    "        # for each line, get a dictionary with keys(token pairs) and values(count)\n",
    "        for m in t:\n",
    "                for n in t:\n",
    "                    if (m,n) not in token_pairs_dic and m != n:\n",
    "                        token_pairs_dic[(m,n)] = 1\n",
    "                    elif (m,n) in token_pairs_dic and m != n:\n",
    "                        token_pairs_dic[(m,n)] += 1\n",
    "\n",
    "# the length of the two dictionaries are the number of distinct tokens and token pairs respectively\n",
    "print(\"The number of distinct tokens is {0}\".format(len(tokens_dic)))\n",
    "print(\"The number of distinct token pairs is {0}\".format(len(token_pairs_dic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Part 2:\n",
    "The Python code below can answer the one-token and two-token queries described above, for 'Shakespeare.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1 or 2 space-separated tokens (return to quit): sorry\n",
      "Input a positive integer frequency threshold: 15\n",
      "  n(sorry) = 91 \n",
      "  high PMI tokens with respect to sorry (threshold: 15):\n",
      "    n('sorry', 'am') = 62,  PMI('sorry', 'am') = 1.5950015984520203\n",
      "    n('sorry', 'i') = 68,  PMI('sorry', 'i') = 0.6906128710555685\n",
      "    n('sorry', 'for') = 20,  PMI('sorry', 'for') = 0.565108851217973\n",
      "    n('sorry', 'that') = 21,  PMI('sorry', 'that') = 0.4271311667155323\n",
      "    n('sorry', 'you') = 16,  PMI('sorry', 'you') = 0.24684833684509966\n",
      "Input 1 or 2 space-separated tokens (return to quit): am sorry\n",
      "  n(am,sorry) = 62 \n",
      "  PMI(am,sorry) = 1.5950015984520203 \n",
      "Input 1 or 2 space-separated tokens (return to quit): am sory\n",
      "  n(am,sory) = 0 \n",
      "There is no such token pair in the text file\n",
      "Input 1 or 2 space-separated tokens (return to quit): i am sorry\n",
      "Input must consist of 1 or 2 space-separated tokens!\n",
      "Input 1 or 2 space-separated tokens (return to quit): \n"
     ]
    }
   ],
   "source": [
    "# this imports the SimpleTokenize function from the simple_tokenize.py file\n",
    "from simple_tokenize import simple_tokenize\n",
    "# the log function for computing PMI\n",
    "# for the sake of consistency across solutions, use log base 10\n",
    "from math import log\n",
    "# numpy is imported to get the distinct tokens of each line\n",
    "import numpy as np\n",
    "\n",
    "###################################################################################################################\n",
    "tokens_lines = {}\n",
    "token_pairs_lines = {}\n",
    "with open('Shakespeare.txt') as f:\n",
    "    for line in f:\n",
    "        # tokenize, one line at a time\n",
    "        t = simple_tokenize(line)\n",
    "        # get the distinct tokens of each line\n",
    "        x = np.array(t)\n",
    "        u, indices = np.unique(x, return_index=True)\n",
    "        t_new = list(u)\n",
    "        # for each line, get a dictionary with keys(tokens) and values(count) \n",
    "        for i in t_new:\n",
    "            if i not in tokens_lines:\n",
    "                tokens_lines[i] = 1\n",
    "            else:\n",
    "                tokens_lines[i] += 1\n",
    "        # for each line, get a dictionary with keys(token pairs) and values(count) \n",
    "        for m in t_new:\n",
    "                for n in t_new:\n",
    "                    if (m,n) not in token_pairs_lines and m != n:\n",
    "                        token_pairs_lines[(m,n)] = 1\n",
    "                    elif (m,n) in token_pairs_lines and m != n:\n",
    "                        token_pairs_lines[(m,n)] += 1\n",
    "# get the number of lines of this text file\n",
    "n_total = len(open('Shakespeare.txt').readlines())\n",
    "###################################################################################################################\n",
    "\n",
    "###################################################################################################################\n",
    "#  the user interface below defines the types of PMI queries that users can ask\n",
    "###################################################################################################################\n",
    "\n",
    "while True:\n",
    "    q = input(\"Input 1 or 2 space-separated tokens (return to quit): \")\n",
    "    if len(q) == 0:\n",
    "        break\n",
    "    q_tokens = simple_tokenize(q)\n",
    "    \n",
    "    if len(q_tokens) == 1:\n",
    "        # if there is no such a token in the text file, print directly\n",
    "        if q_tokens[0] not in tokens_lines:\n",
    "            print(\"  n({0}) = 0 \".format(q_tokens[0]))\n",
    "            print(\"There is no such token in the text file\")\n",
    "        # if there is such a token in the text file, do the following steps\n",
    "        else:\n",
    "            threshold = 0\n",
    "            while threshold <= 0:\n",
    "                try:\n",
    "                    threshold = int(input(\"Input a positive integer frequency threshold: \"))\n",
    "                except ValueError:\n",
    "                    print(\"Threshold must be a positive integer!\")\n",
    "                    continue\n",
    "            # count the number of lines that a given single token appears in the text file\n",
    "            n_0 = tokens_lines.get(q_tokens[0])\n",
    "            # a dictionary storing token pairs that meet certain requirements and the number of lines they appear\n",
    "            tokens_specific_number = {}\n",
    "            # a dictionary storing token pairs that meet certain requirements and their PMI\n",
    "            tokens_specific_PMI = {}\n",
    "            # for each token in the text that is not equal to the given token, calculate the relevant numbers to get PMI\n",
    "            for i in tokens_lines:\n",
    "                if i != q_tokens[0]:\n",
    "                    if token_pairs_lines.get((q_tokens[0],i)) and (token_pairs_lines.get((q_tokens[0],i)) >= threshold):\n",
    "                        n_1 = tokens_lines.get(i)\n",
    "                        n_2 = token_pairs_lines.get((q_tokens[0],i))\n",
    "                        tokens_specific_number[(q_tokens[0],i)] = n_2\n",
    "                        tokens_specific_PMI[(q_tokens[0],i)] = log((n_2/n_total)/((n_0/n_total)*(n_1/n_total)),10)\n",
    "            # sort the 'tokens_specific_PMI' dictionary based on values\n",
    "            PMI_dic_sort = dict(sorted(tokens_specific_PMI.items(), key = lambda item:item[1], reverse = True))\n",
    "            result_1 = list(PMI_dic_sort.keys())\n",
    "            result_2 = list(PMI_dic_sort.values())\n",
    "            result_3 = []\n",
    "            # for each token pair in sorted list, get their number of lines appearing in the text file\n",
    "            for i in range(len(result_1)):\n",
    "                result_3.append(tokens_specific_number[result_1[i]])\n",
    "            print(\"  n({0}) = {1} \".format(q_tokens[0],n_0))\n",
    "            print(\"  high PMI tokens with respect to {0} (threshold: {1}):\".format(q_tokens[0],threshold))\n",
    "            # if there are fewer than 5 such tokens, then report less than 5\n",
    "            for i in range(5):\n",
    "                if i < len(result_1):\n",
    "                    print(\"    n{0} = {1},  PMI{0} = {2}\".format(result_1[i],result_3[i],result_2[i]))   \n",
    "\n",
    "    elif len(q_tokens) == 2:\n",
    "        # if there is no such a token pair, the number of such lines is zero and we could not compute PMI\n",
    "        if (q_tokens[0],q_tokens[1]) not in token_pairs_lines:\n",
    "            print(\"  n({0},{1}) = 0 \".format(q_tokens[0],q_tokens[1]))\n",
    "            print(\"There is no such token pair in the text file\")\n",
    "        # if there is such a token pair, count the number of lines token 1 appears, the number of lines token 2 appears\n",
    "        # also the number of lines the token pair appears. Then we can count PMI of the token pair.\n",
    "        else:\n",
    "            n_0 = tokens_lines.get(q_tokens[0]) \n",
    "            n_1 = tokens_lines.get(q_tokens[1])\n",
    "            n_2 = token_pairs_lines.get((q_tokens[0],q_tokens[1]))\n",
    "            PMI_two_tokens = log((n_2/n_total)/((n_0/n_total)*(n_1/n_total)),10)\n",
    "            print(\"  n({0},{1}) = {2} \".format(q_tokens[0],q_tokens[1],n_2))\n",
    "            print(\"  PMI({0},{1}) = {2} \".format(q_tokens[0],q_tokens[1],PMI_two_tokens))\n",
    "    \n",
    "    else:\n",
    "        print(\"Input must consist of 1 or 2 space-separated tokens!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

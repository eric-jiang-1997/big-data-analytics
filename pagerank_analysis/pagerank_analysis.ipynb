{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "#### This is one of the assignments from course CS 431/631 (Data-intensive Distributed Analytics) at University of Waterloo.\n",
    "#### This assignment focuses on the graph analysis to study the page rank value of a graph.\n",
    "#### Some modifications have been made to improve the presentation on this platform.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "**Goal:** Use Python and Spark to perform some graph analysis, using a graph of the Gnutella server network.   In this graph, each node represents a server, and each (directed) edge represents a connection between servers in Gnutella's peer-to-peer network.  \n",
    "**Files needed:** `p2p-Gnutella08-adj.txt`  \n",
    "(This file represents the graph as an adjacency list.   Each server (node) is identified by a unique number, and each line in the file gives the adjacency list for a single server.\n",
    "For example, this line:\n",
    "> 91\t243\t1923\t2194\n",
    "\n",
    "gives the adjacency list for server `91`.   It indicates that there are edges from server `91` to servers `243`, `1923`, and `2194`.    According to the Stanford Network Analysis Project, which collected these data, [the graph includes 6301 servers and 20777 edges](http://snap.stanford.edu/data/p2p-Gnutella08.html).)\n",
    "\n",
    "For here, use the Spark installation in the CS451 course account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/u/cs451/packages/spark\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then create a `SparkContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sc = SparkContext(appName=\"YourTest\", master=\"local[2]\", conf=SparkConf().set('spark.ui.port', random.randrange(4000,5000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Part 1:\n",
    "\n",
    "To get warmed up, write Spark code to confirm or determine some basic properties of the Gnutella graph.  The code answers the following questions:\n",
    "- How many nodes and edges are there in the graph? \n",
    "- How many nodes of each outdegree are there? That is, how many nodes have no outgoing edges, how many have one outgoing edge, how many have two outgoing edges, and so on?\n",
    "- How many nodes of each indegree are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_nodes_edges():\n",
    "    \"\"\"Returns a tuple (num_nodes, num_edges)\"\"\"\n",
    "    nodes = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "    # get the total number of nodes in the graph\n",
    "    num_nodes = nodes.count()\n",
    "    # split each line to several separate nodes\n",
    "    nodes_new = nodes.map(lambda x:x.split('\\t'))\n",
    "    nodes_and_edges = nodes_new.flatMap(lambda x:x)\n",
    "    # get the total number of edges in the graph\n",
    "    num_edges = nodes_and_edges.count() - num_nodes\n",
    "    return (num_nodes,num_edges)\n",
    "    \n",
    "\n",
    "def out_counts():\n",
    "    \"\"\"Returns a dictionary where the keys are the outdegrees, and the \n",
    "    values are the number of nodes of the corresponding outdegree \"\"\"\n",
    "    nodes = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "    nodes_new = nodes.map(lambda x:x.split('\\t'))\n",
    "    # get the outdegree of each node\n",
    "    out_degree = nodes_new.map(lambda x:(x[0],len(x)-1))\n",
    "    # get the number of nodes of each outdegree, and then collect\n",
    "    aggregated_out_degree = out_degree.map(lambda x:(x[1],x[0])).groupByKey().map(lambda x:(x[0],len(x[1]))).sortByKey().collect()\n",
    "    # transform the output into desired dictionary format\n",
    "    dic_out_degree = dict(aggregated_out_degree)\n",
    "    return dic_out_degree\n",
    "    \n",
    "\n",
    "def in_counts():\n",
    "    \"\"\"Returns a dictionary where the keys are the indegrees, and the \n",
    "    values are the number of nodes of the corresponding indegree \"\"\"\n",
    "    nodes = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "    # split each line to several separate nodes (the adjacent nodes)\n",
    "    nodes_new = nodes.map(lambda x:x.split('\\t')[1:])\n",
    "    # get the indegree of each node\n",
    "    in_degree = nodes_new.flatMap(lambda x:x).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "    # get the number of nodes of each indegree, and then collect\n",
    "    aggregated_in_degree = in_degree.map(lambda x:(x[1],x[0])).groupByKey().map(lambda x:(x[0],len(x[1]))).sortByKey().collect()\n",
    "    # transform the output into desired dictionary format\n",
    "    dic_in_degree = dict(aggregated_in_degree)\n",
    "    return dic_in_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Some background information about page rank:**  \n",
    "The description of ordinary page rank is in Section 5.3 of [the course textbook](http://mapreduce.cc/).   Personalized page rank is like ordinary page rank except:\n",
    "- One node in the graph is designated as the *source* node. Personalized page rank is performed with respect to that source node.\n",
    "- Personalized page rank is initialized by assigning all probability mass to the source node, and none to the other nodes. In contrast, ordinary page rank is initialized by giving all nodes the same probability mass.\n",
    "- Whenever personalized page rank makes a random jump, it jumps back to the source node. In contrast, ordinary page rank may jump to any node.\n",
    "- In personalized page rank, all probability mass lost dangling nodes is put back into the source nodes.  In ordinary page rank, lost mass is distributed evenly over all nodes.\n",
    "\n",
    "#### Part 2:\n",
    "\n",
    "The task is to write a Spark program to perform personalized page rank over the Gnutella graph for a specified number of iterations, and of course a specific node. The function takes three input values:\n",
    "- source node id (a positive integer)\n",
    "- iteration count (a positive integer)\n",
    "- random jump factor value (a float between 0 and 1)\n",
    "\n",
    "\n",
    "The output should be a list of the 10 nodes with the highest personalized page rank with respect to the given source. For each of the 10 nodes, return the node's id and page rank value as a tuple. The list returned by the function should therefore look something like this: `[(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def personalized_page_rank(source_node_id, num_iterations, jump_factor):\n",
    "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
    "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
    "    nodes = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "    # transform the nodes into integer format, to match the input format of 'souce_node_id'\n",
    "    nodes_new = nodes.map(lambda x:[int(i) for i in re.findall(\"[0-9]+\",x)])\n",
    "    \n",
    "    # define a function that returns the original pagerank value of each node\n",
    "    def is_source_node(x):\n",
    "        if (x == source_node_id):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0            \n",
    "    # call the above function to get the original pagerank value of each node\n",
    "    nodes_rank = nodes_new.map(lambda x:(x[0],is_source_node(x[0])))\n",
    "    \n",
    "    # get the adjacency list of each node \n",
    "    \"\"\"Attention: all probability mass lost dangling nodes is put back into the source nodes,\n",
    "       which means the adjaceny list of a dangling node is the source node\"\"\"\n",
    "    nodes_adjacency_list = nodes_new.map(lambda x:(x[0],x[1:]) if x[1:] != [] else (x[0],[source_node_id]))\n",
    "    \n",
    "    # join the above two RDD to get the detailed information of each node\n",
    "    nodes_detailed = nodes_adjacency_list.join(nodes_rank).map(lambda x:(x[0],list(x[1])))\n",
    "    \n",
    "    # define a function that returns the sum of pagerank values from its adjacent nodes\n",
    "    def get_rank_sum(x):\n",
    "        result = []\n",
    "        for i in x[1][0]:\n",
    "            result.append((i,x[1][1]/len(x[1][0])))\n",
    "        return result\n",
    "    \n",
    "    # define a function that takes the jump factor into account\n",
    "    def set_jump_factor(x,source_node_id,jump_factor):\n",
    "        if x[0] == source_node_id:\n",
    "            x[1][1] = jump_factor\n",
    "        else:\n",
    "            x[1][1] = 0\n",
    "        return x\n",
    "    \n",
    "    #### iteration part ####\n",
    "    for i in range(num_iterations):\n",
    "        # call the function 'get_rank_sum' to get the sum of pagerank values from each node's adjacent nodes\n",
    "        # (use the 'repartition' to faster the later 'join' process)\n",
    "        rank_sum = nodes_detailed.flatMap(lambda x:get_rank_sum(x)).reduceByKey(lambda x,y:x+y).repartition(1)\n",
    "        # join 'nodes_detailed' and 'rank_sum', and call the function 'set_jump_factor' to update 'nodes_detailed'\n",
    "        nodes_detailed = nodes_detailed.map(lambda x:set_jump_factor(x,source_node_id,jump_factor)).repartition(1)\\\n",
    "                         .join(rank_sum).map(lambda x:(x[0],[x[1][0][0],x[1][0][1]+(1-jump_factor)*x[1][1]]))\n",
    "    \n",
    "    # extract the desired outcome from the most updated 'nodes_detailed' and sort by values\n",
    "    nodes_rank_sorted = nodes_detailed.map(lambda x:(x[1][1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0]))\n",
    "            \n",
    "    return nodes_rank_sorted.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Part 3:\n",
    "\n",
    "For the previous part, the personalized page rank implementation runs for a specified number of iterations.  However, it is also common to write iterative algorithms that run until some specified termination condition is reached.\n",
    "For example, for page rank, suppose the $p_i(x)$ represents the probability mass assigned to node $x$ after the $i$th iteration of the algorithm.  ($p_0(x)$ is the initial probability mass of node $x$.)   We define the change of $x$'s probability mass on the $i$th iteration as $\\lvert p_i(x)-p_{i-1}(x) \\rvert$.   Then, we can iterate personalized page rank until the maximum (over all nodes) change is less than a specified threshold, i.e, until all nodes' page ranks have converged.\n",
    "\n",
    "The task of this part is to modify the code in part 2 so that it iterates until the \n",
    "maximum node change is less than $\\frac{0.5}{N}$, where $N$ represents the number of nodes in the graph.\n",
    "This version of the function should take only two inputs: the source node id and the random jump factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def personalized_page_rank_stopping_criterion(source_node_id, jump_factor):\n",
    "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
    "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
    "    nodes = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "    # transform the nodes into integer format, to match the input format of 'souce_node_id'\n",
    "    nodes_new = nodes.map(lambda x:[int(i) for i in re.findall(\"[0-9]+\",x)]).cache()\n",
    "    # get the number of nodes for further use\n",
    "    num_nodes = nodes.count()\n",
    "    \n",
    "    # define a function that returns the original pagerank value of each node\n",
    "    def is_source_node(x):\n",
    "        if (x == source_node_id):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0            \n",
    "    # call the above function to get the original pagerank value of each node\n",
    "    nodes_rank = nodes_new.map(lambda x:(x[0],is_source_node(x[0])))\n",
    "    \n",
    "    # get the adjacency list of each node \n",
    "    \"\"\"Attention: all probability mass lost dangling nodes is put back into the source nodes,\n",
    "       which means the adjaceny list of a dangling node is the source node\"\"\"\n",
    "    nodes_adjacency_list = nodes_new.map(lambda x:(x[0],x[1:]) if x[1:] != [] else (x[0],[source_node_id])).cache()\n",
    "    \n",
    "    # join the above two RDD to get the detailed information of each node\n",
    "    nodes_detailed = nodes_adjacency_list.join(nodes_rank).map(lambda x:(x[0],list(x[1]))).cache()\n",
    "    \n",
    "    # define a function that returns the sum of pagerank values from its adjacent nodes\n",
    "    def get_rank_sum(x):\n",
    "        result = []\n",
    "        for i in x[1][0]:\n",
    "            result.append((i,x[1][1]/len(x[1][0])))\n",
    "        return result\n",
    "    \n",
    "    # define a function that takes the jump factor into account\n",
    "    def set_jump_factor(x,source_node_id,jump_factor):\n",
    "        if x[0] == source_node_id:\n",
    "            x[1][1] = jump_factor\n",
    "        else:\n",
    "            x[1][1] = 0\n",
    "        return x\n",
    "    \n",
    "    #### modified iteration part ####\n",
    "    # set the origianl value of 'nodes_value_diff_max' to 1 to start the while loop\n",
    "    nodes_value_diff_max = 1\n",
    "    while(nodes_value_diff_max >= 0.5/num_nodes):\n",
    "        # get the pagerank value of each node before the current iteration\n",
    "        nodes_value_0 = nodes_detailed.map(lambda x:(x[0],x[1][1]))\n",
    "        # call the function 'get_rank_sum' to get the sum of pagerank values from each node's adjacent nodes\n",
    "        # (use the 'repartition' to faster the later 'join' process)\n",
    "        rank_sum = nodes_detailed.flatMap(lambda x:get_rank_sum(x)).reduceByKey(lambda x,y:x+y).repartition(1).cache()\n",
    "        # join 'nodes_detailed' and 'rank_sum', and call the function 'set_jump_factor' to update 'nodes_detailed'\n",
    "        nodes_detailed = nodes_detailed.map(lambda x:set_jump_factor(x,source_node_id,jump_factor)).repartition(1)\\\n",
    "                         .join(rank_sum).map(lambda x:(x[0],[x[1][0][0],x[1][0][1]+(1-jump_factor)*x[1][1]])).cache()\n",
    "        # get the pagerank value of each node after the current iteration\n",
    "        nodes_value_1 = nodes_detailed.map(lambda x:(x[0],x[1][1]))\n",
    "        # join 'nodes_value_0' and 'nodes_value_1' to get the difference of pagerank values after the current iteration\n",
    "        nodes_value_diff = nodes_value_1.join(nodes_value_0).map(lambda x:abs(x[1][0]-x[1][1])).collect()\n",
    "        # get the maximum number of 'nodes_value_diff' to decide whether to continue the while loop or stop\n",
    "        nodes_value_diff_max = max(nodes_value_diff)\n",
    "    \n",
    "    # extract the desired outcome from the most updated 'nodes_detailed' and sort by values\n",
    "    nodes_rank_sorted = nodes_detailed.map(lambda x:(x[1][1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0]))\n",
    "    \n",
    "    return nodes_rank_sorted.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
